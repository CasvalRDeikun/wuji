[config]
seed = 0

[eval]
problem = wuji.problem.mdp.gym.Problem	wuji.problem.mdp.wrap.random.reset_seed()

[gym]
make = PongNoFrameskip-v4
unwrapped = 0
wrapper = wuji.problem.mdp.gym.wrapper.atari.NoopResetEnv(env, 30)	wuji.problem.mdp.gym.wrapper.atari.MaxAndSkipEnv(env, 4)	wuji.problem.mdp.gym.wrapper.atari.wrap_deepmind(env)	wuji.problem.mdp.gym.wrapper.Cast(env)	wuji.problem.mdp.gym.wrapper.Normalize(env, std=255)	wuji.problem.mdp.gym.wrapper.image.StackZero(env, 4)
length = 100

[gym_state]
ram = 1
image = 1

[gym_weight_reward]
env = 1

[gym_weight_final_reward]
env = 1

[model]
root = ~/model/wuji
module = wuji.model.pth.conv.PongDebug
critic = wuji.model.pth.wrap.critic.conv.hidden0
init = wuji.model.pth.wrap.init.kaiming_normal

[train]
optimizer = torch.optim.Adam(params, lr, betas=(0.9, 0.999), eps=1e-8)
lr = 0
lr_ = 0
batch_size = 16
batch_size_ = 0
clip_grad_norm = 0.5

[rl]
discount = 0.99
discount_ = 0
wrap =

[opponent_train]
mode = eval

[dqn]
epsilon = 1
epsilon_ = 0
next_epsilon = max(epsilon - 0.001, 0.05)
capacity = 100000
update = 25
loss = mse_loss
buffer = 100
agent_train =
agent_eval =
wrap = wuji.rl.pth.dqn.wrap.buffer.fifo	wuji.rl.pth.dqn.wrap.double
